{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92366306",
   "metadata": {},
   "source": [
    "# globals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe291ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# utils\n",
    "from utils import actions,arg_max\n",
    "\n",
    "# models & realTime\n",
    "from pytorch_model import PytorchPredictor\n",
    "from keras_model import KerasPredictor\n",
    "# from RealTime_predector import RealTime\n",
    "\n",
    "\n",
    "n_classes = len(actions)\n",
    "\n",
    "WEIGHTS_PATH=os.path.join(\"..\",\"..\",\"sign_language_detection\",\"ensemble\",\"V1\")\n",
    "KERAS_WEIGHTS_PATH = os.path.join(WEIGHTS_PATH,\"keras_weights\",\"V1.h5\")\n",
    "TORCH_WEIGHTS_PATH = os.path.join(WEIGHTS_PATH,\"pytorch_weights.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79c5e0",
   "metadata": {},
   "source": [
    "# pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df37573",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor = PytorchPredictor(path=TORCH_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eeff50",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b356f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n",
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 18:01:01.133383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-10 18:01:01.133924: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-03-10 18:01:01.133964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-03-10 18:01:01.133997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-03-10 18:01:01.134068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-03-10 18:01:01.134128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-03-10 18:01:01.134159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-03-10 18:01:01.134340: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-10 18:01:01.134443: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-10 18:01:01.135171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-10 18:01:01.135406: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-10 18:01:01.135680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-10 18:01:01.135970: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "keras_predictor = KerasPredictor(path=KERAS_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93674779",
   "metadata": {},
   "source": [
    "# Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e36a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "num_hand_marks = 21\n",
    "num_pose_marks = 33\n",
    "\n",
    "pose_selected_landmarks = [\n",
    "    [0,2,5,11,13,15,12,14,16],\n",
    "    [0,2,4,5,8,9,12,13,16,17,20],\n",
    "    [0,2,4,5,8,9,12,13,16,17,20],\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def mediapipe_detection(image,model):\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results\n",
    "    \n",
    "    \n",
    "\n",
    "class RealTime:\n",
    "    def __init__(self, holistic, fsize=(512, 512)):\n",
    "        self.fsize = fsize\n",
    "        self.listed_frames = []\n",
    "        self.holistic = holistic\n",
    "        \n",
    "        # current values\n",
    "        self.frame_left_hand = None\n",
    "        self.frame_right_hand = None\n",
    "        \n",
    "        # previous values\n",
    "        self.last_frame_left_hand = None\n",
    "        self.last_frame_right_hand = None\n",
    "        \n",
    "    def read_frame(self,frame):\n",
    "        \n",
    "        frame = cv2.resize(frame, self.fsize)\n",
    "        image, results = mediapipe_detection(frame, self.holistic)\n",
    "        self.draw_styled_landmarks(image, results)\n",
    "        frame_left_hand, frame_right_hand = self.extract_keypoints(results,left=self.last_frame_left_hand,right=self.last_frame_right_hand)\n",
    "        \n",
    "        \n",
    "        self.frame_left_hand = frame_left_hand.sum().round(2)\n",
    "        self.frame_right_hand = frame_right_hand.sum().round(2)\n",
    "        return frame,image\n",
    "        \n",
    "    \n",
    "    def update_last_frame(self):\n",
    "        self.last_frame_left_hand = self.frame_left_hand\n",
    "        self.last_frame_right_hand = self.frame_right_hand\n",
    "    \n",
    "    def add_listed_frame(self, frame):\n",
    "        self.listed_frames.append(frame)\n",
    "    \n",
    "\n",
    "    def considered_frame(self,right_hand_diff_threshold=0.5, left_hand_diff_threshold=0.5):\n",
    "        try:\n",
    "            right_hand_diff = np.abs(self.last_frame_right_hand - self.frame_right_hand).round(2)\n",
    "            left_hand_diff = np.abs(self.last_frame_left_hand - self.frame_left_hand).round(2)\n",
    "\n",
    "            if right_hand_diff >= right_hand_diff_threshold or left_hand_diff >= left_hand_diff_threshold:\n",
    "                return True\n",
    "            return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "    def extract_keypoints(self, results,left,right):\n",
    "        \n",
    "        # extract left hand\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand = np.array([ [res.x,res.y] for res in results.left_hand_landmarks.landmark ]).flatten()\n",
    "        else:\n",
    "            if type(left) == np.ndarray:\n",
    "                left_hand = left\n",
    "            else:\n",
    "                left_hand = np.zeros(num_hand_marks*2)\n",
    "            \n",
    "            \n",
    "        # extract right hand\n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand = np.array([ [res.x,res.y] for res in results.right_hand_landmarks.landmark ]).flatten()\n",
    "        else:\n",
    "            if type(right) == np.ndarray:\n",
    "                right_hand = right\n",
    "            else:\n",
    "                right_hand = np.zeros(num_hand_marks*2)\n",
    "            \n",
    "        \n",
    "        return left_hand, right_hand\n",
    "\n",
    "    def draw_styled_landmarks(self, image, results):\n",
    "        # Draw pose connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw left hand connections\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw right hand connections  \n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "    \n",
    "    def get_frames_indices(self, frames_no):\n",
    "        self.listed_frames = self.listed_frames[1:]\n",
    "        return np.linspace(0, len(self.listed_frames)-1, frames_no, dtype=np.int16)\n",
    "    \n",
    "    def truncate_listed_frames(self):\n",
    "        self.listed_frames = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.listed_frames[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listed_frames)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return dict({\n",
    "            \"L\": self.frame_left_hand,\n",
    "            \"R\": self.frame_right_hand,\n",
    "            \"L2\": self.last_frame_left_hand,\n",
    "            \"R2\": self.last_frame_right_hand,\n",
    "            \"L-D\": np.abs(self.last_frame_left_hand - self.frame_left_hand).round(2),\n",
    "            \"R-D\": np.abs(self.last_frame_right_hand - self.frame_right_hand).round(2)\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed4fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "fsize = (512,512)\n",
    "\n",
    "real_time = RealTime(holistic, fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594b0e8",
   "metadata": {},
   "source": [
    "# Predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14485b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0, 3]\n",
      "[0, 3, 3]\n",
      "[0, 3, 3, 3]\n",
      "[0, 3, 3, 3, 6]\n",
      "[0, 3, 3, 3, 6, 2]\n",
      "[0, 3, 3, 3, 6, 2, 3]\n",
      "[0, 3, 3, 3, 6, 2, 3, 0]\n",
      "[0, 3, 3, 3, 6, 2, 3, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "def display_sentence(frame,sentence):\n",
    "    cv2.rectangle(frame, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "    cv2.putText(frame, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def dispay_probability(frame,data):\n",
    "    TEXT_COLOR = (0,0,255)\n",
    "    \n",
    "    cv2.putText(frame, \"L:\"+str(data[\"L\"]), (0, 85+0*40), cv2.FONT_HERSHEY_SIMPLEX, 1, TEXT_COLOR, 2, cv2.LINE_8)\n",
    "    cv2.putText(frame, \"R:\"+str(data[\"R\"]), (0, 85+1*40), cv2.FONT_HERSHEY_SIMPLEX, 1, TEXT_COLOR, 2, cv2.LINE_8)\n",
    "\n",
    "    cv2.putText(frame, \"L2:\"+str(data[\"L2\"]), (0, 85+4*40), cv2.FONT_HERSHEY_SIMPLEX, 1, TEXT_COLOR, 2, cv2.LINE_8)\n",
    "    cv2.putText(frame, \"R2:\"+str(data[\"R2\"]), (0, 85+5*40), cv2.FONT_HERSHEY_SIMPLEX, 1, TEXT_COLOR, 2, cv2.LINE_8)\n",
    "\n",
    "\n",
    "    cv2.putText(frame, \"L-D:\"+str(data[\"L-D\"]), (0, 85+8*40), cv2.FONT_HERSHEY_SIMPLEX, 1, TEXT_COLOR, 2, cv2.LINE_8)\n",
    "    cv2.putText(frame, \"R-D:\"+str(data[\"R-D\"]), (0, 85+9*40), cv2.FONT_HERSHEY_SIMPLEX, 1, TEXT_COLOR, 2, cv2.LINE_8)\n",
    "\n",
    "\n",
    "def display_counters(frame,counter,discarded_frames):\n",
    "    cv2.putText(frame, str(counter), (250, 85+5*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "    cv2.putText(frame, str(discarded_frames), (250, 85+6*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "\n",
    "\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "discarded_frames = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Read feed\n",
    "    ret, frame = cap.read()\n",
    "    if(not ret):\n",
    "        break\n",
    "    \n",
    "    frame,image = real_time.read_frame(frame)\n",
    "    \n",
    "\n",
    "    if real_time.considered_frame():\n",
    "        counter += 1\n",
    "        real_time.update_last_frame()\n",
    "        discarded_frames = 0\n",
    "        real_time.add_listed_frame(frame)\n",
    "    else:\n",
    "        discarded_frames += 1\n",
    "        if discarded_frames == 10:\n",
    "            counter = 0\n",
    "            discarded_frames = 0\n",
    "            if len(real_time) >= 16:\n",
    "                frame_list = real_time.get_frames_indices(frames_no=16)\n",
    "\n",
    "                for frame_idx in frame_list:\n",
    "                    pytorch_predictor.add_frame(real_time[frame_idx])\n",
    "                    keras_predictor.add_frame(real_time[frame_idx])\n",
    "\n",
    "                res1 = pytorch_predictor.predict()\n",
    "                res2 = keras_predictor.predict()\n",
    "                res = res1 + res2\n",
    "                arg_max = np.argmax(res)\n",
    "                predictions.append(arg_max)\n",
    "                predictions = predictions[-16:]\n",
    "                print(predictions)\n",
    "                real_time.truncate_listed_frames()\n",
    "                sentence.append(actions[arg_max])\n",
    "                sentence = sentence[-4:]\n",
    "            else:\n",
    "                real_time.truncate_listed_frames()\n",
    "\n",
    "\n",
    "    display_sentence(image,sentence)\n",
    "    dispay_probability(image,real_time.get_data())\n",
    "    display_counters(image,counter,discarded_frames)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Real-Time\", image)\n",
    "\n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071ab9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04717328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
