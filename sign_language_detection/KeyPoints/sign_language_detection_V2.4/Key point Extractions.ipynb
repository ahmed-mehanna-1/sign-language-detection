{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c03c2d",
   "metadata": {},
   "source": [
    "# 1 -  install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bffa97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e681a",
   "metadata": {},
   "source": [
    "# 2 - keypoints extractions and drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d45b88",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "- link to mediapipe documentation and info about keypoints numbers\n",
    "- https://google.github.io/mediapipe/solutions/hands.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- link to mediapipe code for drawing (to draw the point myself)\n",
    "- https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/drawing_utils.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab75ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "pose_selected_landmarks = [\n",
    "    [0,2,5,11,13,15,12,14,16], # responsible for pose \n",
    "    [0,2,4,5,8,9,12,13,16,17,20], # left hand\n",
    "    [0,2,4,5,8,9,12,13,16,17,20], # right hand\n",
    "]\n",
    "\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# holistic model process image and return the results as keypoints\n",
    "def mediapipe_detection(image,model):\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results\n",
    "\n",
    "                \n",
    "                \n",
    "def extract_keypoints(results):\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "    \n",
    "    outputs = []\n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            pose = np.array([ [lis[res].x,lis[res].y] for res in pose_selected_landmarks[shape] ]).flatten()\n",
    "        else:\n",
    "            pose = np.zeros(len(pose_selected_landmarks[shape])*2)\n",
    "        outputs.append(pose)\n",
    "    return np.concatenate([outputs[0],outputs[1],outputs[2]])\n",
    "            \n",
    "\n",
    "\n",
    "def draw_landmark_from_results(image,results):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            for idx in pose_selected_landmarks[shape]:\n",
    "                point = lis[idx]\n",
    "                landmark_px = mp_drawing._normalized_to_pixel_coordinates(point.x, point.y,\n",
    "                                                           image_cols, image_rows)\n",
    "\n",
    "                cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                         4)\n",
    "\n",
    "\n",
    "def draw_landmark_from_array(image,keyPoints):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    \n",
    "    for i in range(len(keyPoints)//2):\n",
    "        x = keyPoints[i*2]\n",
    "        y = keyPoints[i*2+1]\n",
    "        if(x!=0 and y!=0): \n",
    "            landmark_px = mp_drawing._normalized_to_pixel_coordinates(x,y,\n",
    "                                                       image_cols, image_rows)\n",
    "            cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                     4)\n",
    "\n",
    "                \n",
    "\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a53451",
   "metadata": {},
   "source": [
    "# 3 - read and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c63d9",
   "metadata": {},
   "source": [
    "### 3.1 get action list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a26a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../../data/Datasets/\"\n",
    "\n",
    "train_labels = pd.read_csv(os.path.join(data_path,\"train_labels.csv\"),names=['sample','id'])\n",
    "validation_labels = pd.read_csv(os.path.join(data_path,\"validation_labels.csv\"),names=['sample','id'])\n",
    "test_labels = pd.read_csv(os.path.join(data_path,\"test_labels.csv\"),names=['sample','id'])\n",
    "class_id = pd.read_csv(os.path.join(data_path,\"class_id.csv\"))\n",
    "\n",
    "train_path = os.path.join(data_path,\"train\")\n",
    "val_path = os.path.join(data_path,\"val\")\n",
    "test_path = os.path.join(data_path,\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2cef896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 10 signs\n",
    "\n",
    "actions_ids= [\n",
    "    0,1,2,3,4,5,6,7,8,9 # first 10 actions\n",
    "]\n",
    "n_actions = len(actions_ids)\n",
    "\n",
    "actions = list(np.array(class_id['EN'])[actions_ids])\n",
    "actions[3] = \"meal\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96171e",
   "metadata": {},
   "source": [
    "### 3.2 collecting the data pathes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0631ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file(file_path):\n",
    "    try:\n",
    "        f = open(file_path)\n",
    "        f.close()\n",
    "        return True\n",
    "    except IOError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def construct_path(file,data_mode=\"train\"):\n",
    "    return os.path.join(data_path,data_mode,file+\"_color.mp4\")\n",
    "    \n",
    "    \n",
    "def get_data(value,data_mode):\n",
    "\n",
    "    if data_mode==\"train\":\n",
    "        label_dic = train_labels\n",
    "    elif data_mode==\"val\":\n",
    "        label_dic = validation_labels\n",
    "    elif data_mode==\"test\":\n",
    "        label_dic = test_labels\n",
    "        \n",
    "    \n",
    "    data =  label_dic[label_dic['id']==value]\n",
    "    lis =  [construct_path(i,data_mode) for i in  (data['sample'])]\n",
    "    data =  [i for i in lis if check_file(i)]\n",
    "    return data,[value for i in data]\n",
    "\n",
    "\n",
    "def get_one_class(value,data_mode):\n",
    "    if data_mode==\"train\":\n",
    "        label_dic = train_labels\n",
    "    elif data_mode==\"val\":\n",
    "        label_dic = validation_labels\n",
    "    elif data_mode==\"test\":\n",
    "        label_dic = test_labels\n",
    "        \n",
    "    data =  label_dic[label_dic['id']==value]\n",
    "    lis =  [construct_path(i,data_mode) for i in  (data['sample'])]\n",
    "    data =  [i for i in lis if check_file(i)]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def get_list(d,data_mode):\n",
    "    arr_x=[]\n",
    "    arr_y=[]\n",
    "    for index,value in enumerate(d):\n",
    "        \n",
    "        data,labels = get_data(value,data_mode)\n",
    "        arr_x.extend(data)\n",
    "        arr_y.extend(labels)\n",
    "    return arr_x,arr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02e60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241 1241 190 190 168 168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_X,train_Y = get_list(actions_ids,\"train\")\n",
    "val_X,val_Y = get_list(actions_ids,\"val\")\n",
    "test_X,test_Y = get_list(actions_ids,\"test\")\n",
    "\n",
    "\n",
    "print(\n",
    "    len(train_X ),\n",
    "len(train_Y),\n",
    "len(val_X ),\n",
    "len(val_Y),\n",
    "len(test_X ),\n",
    "len(test_Y)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781311b",
   "metadata": {},
   "source": [
    "### 3.2 get frames from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09571583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove later but save it \n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size=32,data=None,labels=None,image_generator=None ):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.image_generator = image_generator\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of batches\n",
    "        ...\n",
    "        return len(self.data) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # returns one batch\n",
    "        new_ind = index*self.batch_size\n",
    "        y = self.labels[new_ind:new_ind+self.batch_size]\n",
    "        X = self.data[new_ind:new_ind+self.batch_size]\n",
    "        if(self.image_generator):\n",
    "            new_X =[]\n",
    "            for video in X:\n",
    "                new_video = []\n",
    "                for frame in video:\n",
    "                    for trans_frame in self.image_generator.flow(np.expand_dims(frame, axis=0)):\n",
    "                        new_video.append(np.squeeze(trans_frame, axis=0))\n",
    "                        break;\n",
    "                new_X.append(new_video)\n",
    "            X = np.array(new_X)\n",
    "\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        perm = np.random.permutation(len(self.data))\n",
    "        self.data = self.data[perm]\n",
    "        self.labels = self.labels[perm]\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# test_ds = CustomDataset(batch_size=32,data=test_data,labels=test_labels)\n",
    "# model.fit(train_ds,validation_data=test_ds,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d602a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import random\n",
    "\n",
    "class VideoProcessing:\n",
    "    def __init__(self,num_frames,transformer=None):\n",
    "        self.transformer = transformer # the datagenerator class\n",
    "        self.num_frames = num_frames   # the num_frames per video\n",
    "        self.seed = random.randint(1,100000000)\n",
    "    \n",
    "    \n",
    "    def change_seed(self):\n",
    "        self.seed = random.randint(1,100000000)\n",
    "    \n",
    "    def transform(self,frame):\n",
    "        for trans_frame in self.transformer.flow(np.expand_dims(frame, axis=0),seed=self.seed):\n",
    "            return np.squeeze(trans_frame.astype(np.uint8), axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def __capture_frames(self,video_path):\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        video_length = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "        count=0\n",
    "        frames = []\n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(frame)\n",
    "            count += 1\n",
    "            if (count > (video_length-1)):\n",
    "                video.release()\n",
    "        video.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "\n",
    "    def get_frames(self,video_path,num_frames):\n",
    "        # collect 2 extra frames and remove one in the beginnign and last one\n",
    "        num_frames+=4\n",
    "        \n",
    "        frames = self.__capture_frames(video_path)\n",
    "        video_length = len(frames)\n",
    "        steps = video_length/num_frames\n",
    "        count=0\n",
    "        new_frames=[]\n",
    "        while count<video_length:\n",
    "            frame = frames[int(count)]\n",
    "            if(self.transformer !=None):\n",
    "                frame = self.transform(frame)\n",
    "            new_frames.append(frame)\n",
    "            count+=steps\n",
    "        \n",
    "        \n",
    "        num_frames-=4\n",
    "        \n",
    "        # return np.array(new_frames[:num_frames])\n",
    "    \n",
    "        return  np.array(new_frames[2:num_frames+2])\n",
    "\n",
    "    \n",
    "    def extract_keypoints_video(self,frames=None,path=None,display_text=None):\n",
    "        self.change_seed()\n",
    "        if(display_text != None ):\n",
    "            print(display_text,end=\"\\r\")\n",
    "\n",
    "        if(frames==None):\n",
    "            frames = self.get_frames(path,self.num_frames)\n",
    "            \n",
    "        output_key_points=[]\n",
    "        output_images=[]\n",
    "\n",
    "        for frame in frames:\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            output_key_points.append(extract_keypoints(results))\n",
    "            output_images.append(image)\n",
    "        return np.array(output_images),np.array(output_key_points)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class VideosProcessing:\n",
    "    def __init__(self,transformer,num_frames):\n",
    "        self.processor = VideoProcessing(transformer=transformer,num_frames=num_frames)\n",
    "        self.num_frames = num_frames\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def stop_transofrmation(self):\n",
    "        self.processor.transformer = None\n",
    "        \n",
    "    def enable_transformation(self):\n",
    "        self.processor.transformer = self.transformer\n",
    "        \n",
    "        \n",
    "    def convert_get_both(self,array):\n",
    "        output = []\n",
    "        frames_output=[]\n",
    "        for index,video in enumerate(array):\n",
    "            display_text = f\"processing video : {index+1}/{len(array)}\"\n",
    "            frames,keypoints = self.processor.extract_keypoints_video(path=video,display_text=display_text)\n",
    "            output.append(keypoints)\n",
    "            frames_output.append(frames)\n",
    "        return np.array(frames_output),np.array(output)\n",
    "        \n",
    "    def convert(self,array):\n",
    "        output = []\n",
    "        for index,video in enumerate(array):\n",
    "            display_text = f\"processing video : {index+1}/{len(array)}\"\n",
    "            frames,keypoints = self.processor.extract_keypoints_video(path=video,display_text=display_text)\n",
    "            output.append(keypoints)\n",
    "        return np.array(output)\n",
    "\n",
    "    \n",
    "    \n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        #horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "video_processing_obj = VideoProcessing(transformer=datagen,num_frames=16)\n",
    "video_list_obj = VideosProcessing(transformer=datagen,num_frames=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd4a1c",
   "metadata": {},
   "source": [
    "### 3.3 test extracted images and frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99b798",
   "metadata": {},
   "source": [
    "#### 3.3.1 get and view keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ac260a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 1/1\r"
     ]
    }
   ],
   "source": [
    "one_class = get_one_class(0,\"train\")\n",
    "# video_list_obj.stop_transofrmation()\n",
    "video_list_obj.enable_transformation()\n",
    "frames_list,keypoints_list = video_list_obj.convert_get_both(one_class[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "bc5c38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 5/5\r"
     ]
    }
   ],
   "source": [
    "data = get_one_class(1,\"test\")[:5] # get 5 videos with class label from training data\n",
    "video_list_obj.stop_transofrmation()\n",
    "frames_list,keypoints_list = video_list_obj.convert_get_both(data) # convert them to kye points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "98ffbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,keypoints = video_processing_obj.extract_keypoints_video(path=train_X[250])\n",
    "\n",
    "for video_num in range(len(frames_list)):\n",
    "    images = frames_list[video_num]\n",
    "    keypoints = keypoints_list[video_num]\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = images[index]\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c42b0a",
   "metadata": {},
   "source": [
    "#### 3.3.2 view keypoints only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ba771efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 5/5\r"
     ]
    }
   ],
   "source": [
    "one_class = get_one_class(5,\"train\")\n",
    "# video_list_obj.stop_transofrmation()\n",
    "video_list_obj.enable_transformation()\n",
    "keypoints_list = video_list_obj.convert(one_class[60:62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1596c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,keypoints = video_processing_obj.extract_keypoints_video(path=train_X[250])\n",
    "\n",
    "for keypoints in keypoints_list:\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = np.zeros((512,512,3))+255\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdc676",
   "metadata": {},
   "source": [
    "#### 3.3.3 view keypoints from numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08494a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fil_keypoints(array):\n",
    "    output = array.copy()\n",
    "    for i in range(2,len(output)):\n",
    "        current_frame = output[i]\n",
    "        prev_prev_frame = output[i-2]\n",
    "        prev_frame = output[i-1]\n",
    "        for index,num in enumerate(current_frame):\n",
    "            if num==0:\n",
    "                current_frame[index] = prev_frame[index]*2 - prev_prev_frame[index]\n",
    "                \n",
    "    return output\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "898fe74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints_list = np.load(os.path.join(\"key_points\",\"val\",'1.npy'))\n",
    "\n",
    "test_path,_ =  get_list(actions_ids,\"val\")\n",
    "\n",
    "keypoints_list = val_X[:15] # use thing after loading text_X from keypoints directory\n",
    "new_video_processing = VideoProcessing(transformer=None,num_frames=16)\n",
    "for video_index,keypoints in enumerate(keypoints_list):\n",
    "    images = new_video_processing.get_frames(test_path[video_index],16)\n",
    "    new_keypoints = fil_keypoints(keypoints)\n",
    "    for index in range(16):\n",
    "\n",
    "        image = images[index]\n",
    "\n",
    "        keypoint = new_keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4960940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_list = np.load(os.path.join(\"key_points\",\"val\",'1.npy'))\n",
    "\n",
    "for keypoints in keypoints_list:\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = np.zeros((512,512,3))+255\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a2bce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4424e1",
   "metadata": {},
   "source": [
    "# 4 - extract keypoint and save them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5f753",
   "metadata": {},
   "source": [
    "### 4.1  extract training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b5ad3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0                                         \n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "iteration : 1                                         \n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "iteration : 2                                         \n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "iteration : 3                                         \n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "iteration : 4                                         \n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "iteration : 5                                         \n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n"
     ]
    }
   ],
   "source": [
    "# collect realdata and 5 different transformations\n",
    "num_training_iterations = 6\n",
    "\n",
    "for transformation_index in range(num_training_iterations):\n",
    "    dir_name = os.path.join(\"key_points\",\"train\",str(transformation_index))\n",
    "    try:\n",
    "        os.mkdir(dir_name)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"iteration :\",transformation_index,\" \"*40)\n",
    "    \n",
    "    \n",
    "    if(transformation_index == 0):\n",
    "        video_list_obj.stop_transofrmation()\n",
    "    else:\n",
    "        video_list_obj.enable_transformation()\n",
    "        \n",
    "    \n",
    "    for label in actions_id:\n",
    "        \n",
    "        path = os.path.join(dir_name,str(label)+\".npy\")\n",
    "        \n",
    "        if check_file(path):\n",
    "            print(\"Label :\",label,\"already exists\")\n",
    "            continue\n",
    "            \n",
    "        print(\"Label\",label,\" \"*40)\n",
    "        data = get_one_class(label,\"train\") # get videos with class label from training data\n",
    "        data = video_list_obj.convert(data) # convert them to kye points\n",
    "        np.save(path,data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb495eb",
   "metadata": {},
   "source": [
    "### 4.2  extract validation & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7118b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode : val\n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "val end                                         \n",
      "Mode : test\n",
      "Label : 0 already exists\n",
      "Label : 1 already exists\n",
      "Label : 2 already exists\n",
      "Label : 3 already exists\n",
      "Label : 4 already exists\n",
      "Label : 5 already exists\n",
      "Label : 6 already exists\n",
      "Label : 7 already exists\n",
      "Label : 8 already exists\n",
      "Label : 9 already exists\n",
      "test end                                         \n"
     ]
    }
   ],
   "source": [
    "# collect realdata and 5 different transformations\n",
    "\n",
    "video_list_obj.stop_transofrmation()\n",
    "\n",
    "for mode in ['val','test']:\n",
    "    dir_name = os.path.join(\"key_points\",mode)\n",
    "    try:\n",
    "        os.mkdir(dir_name)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Mode :\",mode)\n",
    "    \n",
    "        \n",
    "    \n",
    "    for label in actions_id:\n",
    "        \n",
    "        path = os.path.join(dir_name,str(label)+\".npy\")\n",
    "        \n",
    "        if check_file(path):\n",
    "            print(\"Label :\",label,\"already exists\")\n",
    "            continue\n",
    "        print(\"Label\",label,\" \"*40)\n",
    "        data = get_one_class(label,mode) # get videos with class label from training data\n",
    "        data = video_list_obj.convert(data) # convert them to kye points\n",
    "        np.save(path,data)\n",
    "    print(mode,\"end\",\" \"*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d55d0",
   "metadata": {},
   "source": [
    "### 4.3 load and test all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0e8edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both depends on actions_id -> [0,1,2,3,4,5,6,7,8,9]\n",
    "def load_dir(dir_name,data_temp=None,labels_temp=None,actions_id=None):\n",
    "    if actions_id == None or action_id == \"all\":\n",
    "        actions_id = [int(s.split('.')[0]) for s in os.listdir(os.path.join(dir_name))]\n",
    "        actions_id.sort()\n",
    "    for action_id in actions_id:\n",
    "        new_array = np.load(os.path.join(dir_name,f\"{action_id}.npy\"))\n",
    "        labels_array = np.array([action_id]*len(new_array))\n",
    "\n",
    "        if(type(data_temp) == np.ndarray):\n",
    "            data_temp = np.concatenate([data_temp,new_array])\n",
    "            labels_temp = np.concatenate([labels_temp,labels_array])\n",
    "        else:\n",
    "            data_temp = new_array\n",
    "            labels_temp = labels_array\n",
    "    \n",
    "    return data_temp,labels_temp\n",
    "\n",
    "def load_mul_dir(parent_dir):\n",
    "    data_temp = None\n",
    "    labels_temp = None\n",
    "    for transformation_index in range(len(os.listdir(parent_dir))):\n",
    "        dir_name = os.path.join(parent_dir,str(transformation_index))\n",
    "        data_temp,labels_temp = load_dir(dir_name,data_temp,labels_temp)\n",
    "    return data_temp,labels_temp\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b6ed89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y = load_mul_dir(os.path.join(\"key_points\",\"train\"))\n",
    "val_X,val_Y = load_dir(os.path.join(\"key_points\",\"val\"))\n",
    "test_X,test_Y = load_dir(os.path.join(\"key_points\",\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1b1216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7446, 16, 62) (7446,) (190, 16, 62) (190,) (168, 16, 62) (168,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "train_X.shape,\n",
    "    train_Y.shape,\n",
    "    val_X.shape,\n",
    "    val_Y.shape,\n",
    "    test_X.shape,\n",
    "    test_Y.shape\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0f0f8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test video on train_x\n",
    "keypoints_list = train_x\n",
    "for keypoints in keypoints_list:\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = np.zeros((512,512,3))+255\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de803f4",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f76327",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(train_data))\n",
    "\n",
    "train_X = train_data[perm]\n",
    "train_Y = train_labels[perm]\n",
    "val_X = val_data\n",
    "val_Y = val_labels\n",
    "test_X = test_data\n",
    "test_Y = test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "10087b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4482, 20, 62) (4482,) (118, 20, 62) (118,) (100, 20, 62) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "train_X.shape,\n",
    "train_Y.shape,\n",
    "val_X.shape,\n",
    "val_Y.shape,\n",
    "test_X.shape,\n",
    "test_Y.shape,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
