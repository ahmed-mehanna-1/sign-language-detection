{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92366306",
   "metadata": {},
   "source": [
    "# globals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe291ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "actions = ['sister','hurry','hungry','meal','brother','tree','heavy','cry','family','wise']\n",
    "colors = [\n",
    "    (245,117,16),\n",
    "    (117,245,16),\n",
    "    (16,117,245)\n",
    "]\n",
    "\n",
    "def softmax(x):    \n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "\n",
    "def arg_max(array):\n",
    "    arg_max = np.argmax(array)\n",
    "    return arg_max,array[arg_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79c5e0",
   "metadata": {},
   "source": [
    "# pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd2ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def convert_relu_to_swish(model: nn.Module):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.SiLU(True))\n",
    "        else:\n",
    "            convert_relu_to_swish(child)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "class Swish(nn.Module):\n",
    "    def __init(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mult_(torch.sigmoid(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "class r2plus1d_18(nn.Module):\n",
    "    def __init__(self, pretrained=True, n_classes=3, dropout_p=0.5):\n",
    "        super(r2plus1d_18, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        model = torchvision.models.video.r2plus1d_18(pretrained=self.pretrained)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.r2plus1d_18 = nn.Sequential(*modules)\n",
    "        convert_relu_to_swish(self.r2plus1d_18)\n",
    "        self.fc1 = nn.Linear(model.fc.in_features, self.n_classes)\n",
    "        self.dropout = nn.Dropout(dropout_p, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, f, c, h, w) = x.size()\n",
    "        # x = x.view(b, c, f, h, w)\n",
    "\n",
    "        out = self.r2plus1d_18(x)\n",
    "        out = out.flatten(1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "h, w = 128, 128\n",
    "mean = [0.43216, 0.394666, 0.37645]\n",
    "std = [0.22803, 0.22145, 0.216989]\n",
    "\n",
    "\n",
    "\n",
    "pytorch_model = r2plus1d_18(pretrained=False, n_classes=n_classes)\n",
    "best_checkpoint = torch.load(\"final_weights\\checkpoint_3dcnn_c10_v36.tar\")\n",
    "pytorch_model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pytorch_model = pytorch_model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df37573",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform   = transforms.Resize((h, w))\n",
    "totensor_transform  = transforms.ToTensor()\n",
    "normalize_transform = transforms.Normalize(mean, std)\n",
    "\n",
    "class PytorchPredictor:\n",
    "    def __init__(self,model,device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.sequence = []\n",
    "        \n",
    "    \n",
    "    def can_predict(self):\n",
    "        return len(self.sequence) == 16\n",
    "    \n",
    "    def add_frame(self,frame):\n",
    "        \n",
    "        new_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        new_frame = Image.fromarray(new_frame)\n",
    "        new_frame = resize_transform(new_frame)\n",
    "        new_frame = totensor_transform(new_frame)\n",
    "        new_frame = normalize_transform(new_frame).to(self.device)\n",
    "        \n",
    "        self.sequence.append(new_frame)\n",
    "        self.sequence = self.sequence[-16:]\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        seq = torch.stack(self.sequence).to(self.device)\n",
    "        seq = torch.unsqueeze(seq, dim=0).permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            res = self.model(seq)\n",
    "            res = res.cpu().detach().numpy()[0]\n",
    "            return softmax(res)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eeff50",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b356f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800872b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1be7822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e82d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "num_hand_marks = 21\n",
    "num_pose_marks = 33\n",
    "\n",
    "\n",
    "pose_selected_landmarks = [\n",
    "    [0,2,5,11,13,15,12,14,16],\n",
    "    [0,2,4,5,8,9,12,13,16,17,20],\n",
    "    [0,2,4,5,8,9,12,13,16,17,20],\n",
    "]\n",
    "\n",
    "def draw_updated_styled(image,results):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "\n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            for idx in pose_selected_landmarks[shape]:\n",
    "                point = lis[idx]\n",
    "                landmark_px = mp_drawing._normalized_to_pixel_coordinates(point.x, point.y,\n",
    "                                                           image_cols, image_rows)\n",
    "\n",
    "                cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                         4)\n",
    "                \n",
    "def extract_keypoints(results):\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "    \n",
    "    outputs = []\n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            pose = np.array([ [lis[res].x,lis[res].y] for res in pose_selected_landmarks[shape] ]).flatten()\n",
    "        else:\n",
    "            pose = np.zeros(len(pose_selected_landmarks[shape])*2)\n",
    "        outputs.append(pose)\n",
    "    return np.concatenate([outputs[0],outputs[1],outputs[2]])\n",
    "\n",
    "\n",
    "# holistic model process image and return the results as keypoints\n",
    "def mediapipe_detection(image,model):\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def draw_landmark_from_array(image, keyPoints):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    \n",
    "    for i in range(len(keyPoints)//2):\n",
    "        x = keyPoints[i*2]\n",
    "        y = keyPoints[i*2+1]\n",
    "        if(x!=0 and y!=0): \n",
    "            landmark_px = mp_drawing._normalized_to_pixel_coordinates(x,y,\n",
    "                                                       image_cols, image_rows)\n",
    "            cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                     4)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8766b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input_layer = Input(shape=(16,62))\n",
    "    layer = LSTM(64,return_sequences=True,activation=\"relu\")(input_layer)\n",
    "    layer = LSTM(128,return_sequences=True,activation=\"relu\")(layer)\n",
    "    layer = LSTM(96,return_sequences=False,activation=\"relu\")(layer)\n",
    "    layer = Dense(64,activation=\"relu\")(layer)\n",
    "    layer = Dense(len(actions),activation=\"softmax\")(layer)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=input_layer,outputs=layer)\n",
    "    model.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "keras_weights_dir = os.path.join(\"final_weights\")\n",
    "best_model_file_name = os.path.join(keras_weights_dir,\"V1.h5\")\n",
    "keras_model = get_model()\n",
    "keras_model.load_weights(os.path.join(best_model_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e6af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasPredictor:\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        self.sequence = []\n",
    "        \n",
    "    \n",
    "    def can_predict(self):\n",
    "        return len(self.sequence) == 16\n",
    "    \n",
    "    def add_frame(self,frame):\n",
    "        \n",
    "        f2 = cv2.resize(frame,(512,512))\n",
    "        image, results = mediapipe_detection(f2, holistic)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        \n",
    "        self.sequence.append(keypoints)\n",
    "        self.sequence = self.sequence[-16:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        return self.model.predict(np.expand_dims(self.sequence, axis=0))[0]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72265bc",
   "metadata": {},
   "source": [
    "# Real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e003615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Real_Time:\n",
    "    def __init__(self, cap, holistic, fsize=(512, 512)):\n",
    "        self.cap = cap\n",
    "        self.fsize = fsize\n",
    "        self.listed_frames = []\n",
    "        self.holistic = holistic\n",
    "        self.frame_pose = None\n",
    "        self.frame_left_hand = None\n",
    "        self.frame_right_hand = None\n",
    "        self.last_frame_pose = None\n",
    "        self.last_frame_left_hand = None\n",
    "        self.last_frame_right_hand = None\n",
    "        \n",
    "    def read_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return None, ret, None\n",
    "        frame = cv2.resize(frame, self.fsize)\n",
    "        image, results = mediapipe_detection(frame, self.holistic)\n",
    "        self.draw_styled_landmarks(image, results)\n",
    "        frame_pose, frame_left_hand, frame_right_hand = self.extract_keypoints(results)\n",
    "        self.frame_pose = frame_pose.sum().round(2)\n",
    "        self.frame_left_hand = frame_left_hand.sum().round(2)\n",
    "        self.frame_right_hand = frame_right_hand.sum().round(2)\n",
    "        return frame, ret, image\n",
    "    \n",
    "    def update_last_frame(self):\n",
    "        self.last_frame_pose = self.frame_pose\n",
    "        self.last_frame_left_hand = self.frame_left_hand\n",
    "        self.last_frame_right_hand = self.frame_right_hand\n",
    "    \n",
    "    def add_listed_frame(self, frame):\n",
    "        self.listed_frames.append(frame)\n",
    "    \n",
    "    def considered_frame(self, pose_diff_threshold=0.5, right_hand_diff_threshold=0.5, left_hand_diff_threshold=0.5):\n",
    "        pose_diff = np.abs(self.last_frame_pose - self.frame_pose).round(2)\n",
    "        right_hand_diff = np.abs(self.last_frame_right_hand - self.frame_right_hand).round(2)\n",
    "        left_hand_diff = np.abs(self.last_frame_left_hand - self.frame_left_hand).round(2)\n",
    "\n",
    "        if pose_diff >= pose_diff_threshold or right_hand_diff >= right_hand_diff_threshold or left_hand_diff >= left_hand_diff_threshold:\n",
    "            return True\n",
    "        elif pose_diff < pose_diff_threshold and right_hand_diff < right_hand_diff_threshold and left_hand_diff < left_hand_diff_threshold:\n",
    "            return False\n",
    "\n",
    "    def extract_keypoints(self, results):\n",
    "        # extract pose marks\n",
    "        if results.pose_landmarks:\n",
    "            pose = np.array([ [res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark ]).flatten()\n",
    "        else:\n",
    "            pose = np.zeros(num_pose_marks*4)\n",
    "        \n",
    "        # extract left hand\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand = np.array([ [res.x,res.y] for res in results.left_hand_landmarks.landmark ]).flatten()\n",
    "        else:\n",
    "            left_hand = np.zeros(num_hand_marks*2)\n",
    "            \n",
    "            \n",
    "        # extract right hand\n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand = np.array([ [res.x,res.y] for res in results.right_hand_landmarks.landmark ]).flatten()\n",
    "        else:\n",
    "            right_hand = np.zeros(num_hand_marks*2)\n",
    "        \n",
    "        return pose, left_hand, right_hand\n",
    "\n",
    "    def draw_styled_landmarks(self, image, results):\n",
    "        # Draw pose connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw left hand connections\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "        # Draw right hand connections  \n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                ) \n",
    "    \n",
    "    def get_frames_indices(self, frames_no):\n",
    "        self.listed_frames = self.listed_frames[1:]\n",
    "        return np.linspace(0, len(self.listed_frames)-1, frames_no, dtype=np.int16)\n",
    "    \n",
    "    def truncate_listed_frames(self):\n",
    "        self.listed_frames = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.listed_frames[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listed_frames)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return dict({\n",
    "            \"P\": self.frame_pose,\n",
    "            \"LH\": self.frame_left_hand,\n",
    "            \"RH\": self.frame_right_hand,\n",
    "            \"LP\": self.last_frame_pose,\n",
    "            \"LLH\": self.last_frame_left_hand,\n",
    "            \"LRH\": self.last_frame_right_hand,\n",
    "            \"LP-P\": np.abs(self.last_frame_pose - self.frame_pose).round(2),\n",
    "            \"LLH-LH\": np.abs(self.last_frame_right_hand - self.frame_right_hand).round(2),\n",
    "            \"LRH-RH\": np.abs(self.last_frame_left_hand - self.frame_left_hand).round(2)\n",
    "        })\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    l = len(colors)\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        prob = max(0,prob)\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100)*5, 90+num*40), colors[num%l], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14485b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor = PytorchPredictor(model=pytorch_model,device=device)\n",
    "keras_predictor = KerasPredictor(model=keras_model)\n",
    "\n",
    "sentence = []\n",
    "predictions = []\n",
    "holistic = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "real_time = Real_Time(cap, holistic, fsize=(512, 512))\n",
    "# Set mediapipe model \n",
    "\n",
    "real_time.read_frame()\n",
    "real_time.update_last_frame()\n",
    "\n",
    "counter = 0\n",
    "discarded_frames = 0\n",
    "\n",
    "with holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        frame, ret, image = real_time.read_frame()\n",
    "        if(not ret):\n",
    "            break\n",
    "\n",
    "        if real_time.considered_frame():\n",
    "            counter += 1\n",
    "            update_flag = True\n",
    "            discarded_frames = 0\n",
    "            real_time.add_listed_frame(frame)\n",
    "\n",
    "        elif not real_time.considered_frame(0.7):\n",
    "            discarded_frames += 1\n",
    "            if discarded_frames == 3:\n",
    "                counter = 0\n",
    "                discarded_frames = 0\n",
    "                if len(real_time) >= 16:\n",
    "                    frame_list = real_time.get_frames_indices(frames_no=16)\n",
    "                    \n",
    "                    for frame_idx in frame_list:\n",
    "                        pytorch_predictor.add_frame(real_time[frame_idx])\n",
    "                        keras_predictor.add_frame(real_time[frame_idx])\n",
    "\n",
    "                    res1 = pytorch_predictor.predict()\n",
    "                    res2 = keras_predictor.predict()\n",
    "                    res = res1 + res2\n",
    "                    arg_max = np.argmax(res)\n",
    "                    predictions.append(arg_max)\n",
    "                    predictions = predictions[-16:]\n",
    "                    print(predictions)\n",
    "                    real_time.truncate_listed_frames()\n",
    "                    sentence.append(actions[arg_max])\n",
    "                    \n",
    "                    if len(sentence) > 4:\n",
    "                        sentence = sentence[-4:]\n",
    "\n",
    "        data = real_time.get_data()\n",
    "        frame = image\n",
    "\n",
    "        cv2.rectangle(frame, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(frame, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(frame, \"LRH:\"+str(data[\"LRH\"]), (0, 85+0*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, \"RH:\"+str(data[\"RH\"]), (0, 85+1*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, \"LRH-RH:\"+str(data[\"LRH-RH\"]), (0, 85+2*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "\n",
    "        cv2.putText(frame, \"LLH:\"+str(data[\"LLH\"]), (0, 85+4*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, \"LH:\"+str(data[\"LH\"]), (0, 85+5*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, \"LLH-LH:\"+str(data[\"LLH-LH\"]), (0, 85+6*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "\n",
    "        cv2.putText(frame, \"LP:\"+str(data[\"LP\"]), (0, 85+8*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, \"P:\"+str(data[\"P\"]), (0, 85+9*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, \"LP-P:\"+str(data[\"LP-P\"]), (0, 85+10*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "\n",
    "        cv2.putText(frame, str(counter), (250, 85+5*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "        cv2.putText(frame, str(discarded_frames), (250, 85+6*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,250,150), 2, cv2.LINE_8)\n",
    "\n",
    "        # Updating\n",
    "        if update_flag:\n",
    "            real_time.update_last_frame()\n",
    "        update_flag = False\n",
    "        cv2.imshow(\"Real-Time\", frame)\n",
    "\n",
    "        if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "071ab9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
