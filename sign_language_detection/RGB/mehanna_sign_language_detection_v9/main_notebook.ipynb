{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_frames(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    video_length = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "    count = 0\n",
    "    frames = []\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "        count += 1\n",
    "        if count > (video_length - 1):\n",
    "            video.release()\n",
    "    video.release()\n",
    "    return np.array(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path, num_frames=15, dim=(512, 512)):\n",
    "    frames = capture_frames(video_path)\n",
    "    video_length = len(frames)\n",
    "    steps = video_length/num_frames\n",
    "    count = 0\n",
    "    new_frames = []\n",
    "    while count < video_length:\n",
    "        frame = frames[int(count)]\n",
    "        frame = cv2.resize(frame, dim)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        new_frames.append(frame)\n",
    "        count += steps\n",
    "    \n",
    "    return np.array(new_frames[:num_frames])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# from image_processing import get_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignsFrames(Dataset):\n",
    "    def __init__(self, \n",
    "                data_path,\n",
    "                label_path,\n",
    "                video_type=\"color\",\n",
    "                n_frames=16,\n",
    "                img_size=(512, 512),\n",
    "                n_channels=3,\n",
    "                n_classes=226,\n",
    "                transform=None,\n",
    "                target_transform=None):\n",
    "        super(SignsFrames, self).__init__()\n",
    "        self.n_frames = n_frames\n",
    "        self.img_size = img_size\n",
    "        self.n_channels = n_channels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        labels_file = pd.read_csv(label_path, names=[\"signerX_sampleY\", \"sample_id\"])\n",
    "        self.labels = labels_file.iloc[:, 1]\n",
    "        self.samples = data_path + labels_file.iloc[:, 0] + \"_\" + video_type + \".mp4\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        np_frames = get_frames(self.samples[idx], num_frames=self.n_frames, dim=self.img_size)\n",
    "        frames = torch.empty((self.n_frames, self.n_channels, *self.img_size))\n",
    "        if self.transform:\n",
    "            for i in range(self.n_frames):\n",
    "                frames[i] = self.transform(np_frames[i])\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(self.labels[idx])\n",
    "\n",
    "        (f, c, h, w) = frames.size()\n",
    "        frames = frames.view(c, f, h, w)\n",
    "        \n",
    "        return frames, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Common***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, epoch, device, log_interval):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    losses = []\n",
    "    all_label = []\n",
    "    all_pred = []\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        if isinstance(pred, list):\n",
    "            pred = pred[0]\n",
    "\n",
    "        loss = loss_fn(pred, y.squeeze())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        prediction = torch.max(pred, 1)[1]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            loss, current = loss.item(), batch_idx * len(X)\n",
    "            print(f\"loss: {loss:>7f}  |  [{current:5d}/{size:>5d}]\")\n",
    "\n",
    "        training_loss = sum(losses) / len(losses)\n",
    "        print(\"Average Training Loss of Epoch {}: {:.6f}\".format(epoch + 1, training_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, criterion, dataloader, device, epoch, phase='Train', exp_name = None):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_label = []\n",
    "    all_pred = []\n",
    "    score_frag = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            # get the inputs and labels\n",
    "            inputs_clips, labels = data['data'].to(device), data['label'].to(device)\n",
    "            # forward\n",
    "            outputs_clips = []\n",
    "            for i_clip in range(inputs_clips.size(1)):\n",
    "                inputs = inputs_clips[:,i_clip,:,:]\n",
    "                outputs_clips.append(model(inputs))\n",
    "                # if isinstance(outputs, list):\n",
    "                #     outputs = outputs[0]\n",
    "            outputs = torch.mean(torch.stack(outputs_clips, dim=0), dim=0)\n",
    "            if phase == 'Test':\n",
    "                score_frag.append(outputs.data.cpu().numpy())\n",
    "            # compute the loss\n",
    "            loss = criterion(outputs, labels.squeeze())\n",
    "            losses.append(loss.item())\n",
    "            # collect labels & prediction\n",
    "            prediction = torch.max(outputs, 1)[1]\n",
    "            all_label.extend(labels.squeeze())\n",
    "            all_pred.extend(prediction)\n",
    "            if phase == 'Test':\n",
    "                score = np.concatenate(score_frag)\n",
    "\n",
    "        # Compute the average loss & accuracy\n",
    "        validation_loss = sum(losses)/len(losses)\n",
    "        all_label = torch.stack(all_label, dim=0)\n",
    "        all_pred = torch.stack(all_pred, dim=0)\n",
    "        validation_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    if phase == 'Test':\n",
    "        with open('./results/{}/results_epoch{:03d}_{}.pkl'.format(exp_name, epoch+1, validation_acc), 'wb') as f:\n",
    "            score_dict = dict(zip(dataloader.dataset.sample_names, score))\n",
    "            pickle.dump(score_dict, f)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Models***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Conv3D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relu_to_swish(model: nn.Module):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.SiLU(True))\n",
    "        else:\n",
    "            convert_relu_to_swish(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mult_(torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class r2plus1d_18(nn.Module):\n",
    "    def __init__(self, pretrained=True, n_classes=226, dropout_p=0.5):\n",
    "        super(r2plus1d_18, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        model = torchvision.models.video.r2plus1d_18(pretrained=self.pretrained)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.r2plus1d_18 = nn.Sequential(*modules)\n",
    "        convert_relu_to_swish(self.r2plus1d_18)\n",
    "        self.fc1 = nn.Linear(model.fc.in_features, self.n_classes)\n",
    "        self.dropout = nn.Dropout(dropout_p, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (b, f, c, h, w) = x.size()\n",
    "        x = x.view(b, c, f, h, w)\n",
    "\n",
    "        out = self.r2plus1d_18(x)\n",
    "        out = out.flatten(1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from dataset import SignsFrames\n",
    "# from models.Conv3D import r2plus1d_18\n",
    "# from models.common import train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = \"rgb_final\"\n",
    "model_path = f\"checkpoint/{exp_name}\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "if not os.path.exists(os.path.join(\"results\", exp_name)):\n",
    "    os.mkdir(os.path.join(\"results\", exp_name))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((3, 3)).to(device)\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Datasets/AUTSL/val/\"\n",
    "label_path = \"../Datasets/AUTSL/labels/val_labels.csv\"\n",
    "n_classes = 226\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "log_interval = 80\n",
    "sample_size = 128\n",
    "drop_p = 0.0\n",
    "hidden1, hidden2 = 512, 256\n",
    "\n",
    "params = {\n",
    "    \"data_path\": data_path,\n",
    "    \"label_path\": label_path,\n",
    "    \"video_type\": \"color\",\n",
    "    \"n_frames\": 10,\n",
    "    \"img_size\": (128, 128),\n",
    "    \"n_channels\": 3,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"transform\": ToTensor(),\n",
    "    \"target_transform\": Lambda(lambda y: torch.zeros(n_classes, dtype=torch.float).scatter(0, torch.tensor(y), value=1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 10, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "autsl_dt = SignsFrames(**params)\n",
    "frames, label = autsl_dt[0]\n",
    "print(type(frames))\n",
    "print(frames.size())\n",
    "train_dl = DataLoader(autsl_dt, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18324/3158238658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mval_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18324/2707189140.py\u001b[0m in \u001b[0;36mval_epoch\u001b[1;34m(model, criterion, dataloader, device, epoch, phase, exp_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m# get the inputs and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0minputs_clips\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0moutputs_clips\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.video.r2plus1d_18(pretrained=True, progress=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    val_epoch(model, loss_fn, train_dl, device, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = r2plus1d_18(pretrained=True, n_classes=500)\n",
    "checkpoint = torch.load(\"pretrained/slr_resnet2d+1.pth\")\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.fc1 = nn.Linear(model.fc1.in_features, n_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"lr: \", get_lr(optimizer))\n",
    "    train_epoch(model, train_dl, loss_fn, optimizer, epoch, device, log_interval)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f\"sign_resnet2d+1_epoch{epoch + 1:03d}.pth\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  250395 KB |  376175 KB |  500686 KB |  250290 KB |\n",
      "|       from large pool |  239396 KB |  360080 KB |  478687 KB |  239291 KB |\n",
      "|       from small pool |   10999 KB |   16644 KB |   21999 KB |   10999 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  250395 KB |  376175 KB |  500686 KB |  250290 KB |\n",
      "|       from large pool |  239396 KB |  360080 KB |  478687 KB |  239291 KB |\n",
      "|       from small pool |   10999 KB |   16644 KB |   21999 KB |   10999 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  393216 KB |  393216 KB |  393216 KB |       0 B  |\n",
      "|       from large pool |  374784 KB |  374784 KB |  374784 KB |       0 B  |\n",
      "|       from small pool |   18432 KB |   18432 KB |   18432 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  142820 KB |  142820 KB |  427846 KB |  285026 KB |\n",
      "|       from large pool |  135388 KB |  135388 KB |  402086 KB |  266698 KB |\n",
      "|       from small pool |    7432 KB |    7562 KB |   25760 KB |   18328 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     449    |     562    |     898    |     449    |\n",
      "|       from large pool |      38    |      57    |      76    |      38    |\n",
      "|       from small pool |     411    |     505    |     822    |     411    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     449    |     562    |     898    |     449    |\n",
      "|       from large pool |      38    |      57    |      76    |      38    |\n",
      "|       from small pool |     411    |     505    |     822    |     411    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      27    |       0    |\n",
      "|       from large pool |      18    |      18    |      18    |       0    |\n",
      "|       from small pool |       9    |       9    |       9    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      56    |      57    |     129    |      73    |\n",
      "|       from large pool |      13    |      13    |      35    |      22    |\n",
      "|       from small pool |      43    |      44    |      94    |      51    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5442513ba35897157162e772485767663a309d0b481473f29ae9b3d4e5ea1118"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('slr_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
